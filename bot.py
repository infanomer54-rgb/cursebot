# –§–∞–π–ª bot.py - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –±–æ—Ç–∞
import os
import logging
import sqlite3
import re
import asyncio
from datetime import datetime
import json
import io
import sys
import random
import hashlib
from typing import List, Dict
from collections import Counter
from threading import Thread

import requests
import PyPDF2
import docx2txt
import aiofiles
from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from bs4 import BeautifulSoup
from googlesearch import search
from transformers import pipeline
from sentence_transformers import SentenceTransformer
import textstat
import pymorphy3
from flask import Flask

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application, CommandHandler, MessageHandler, CallbackQueryHandler,
    ContextTypes, filters
)

# Flask app –¥–ª—è Railway
app = Flask(__name__)

@app.route('/')
def home():
    return "ü§ñ Academic Writing Bot is running!"

@app.route('/health')
def health():
    return "OK", 200

def run_flask():
    port = int(os.getenv("PORT", 8080))
    app.run(host='0.0.0.0', port=port)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BOT_TOKEN = os.getenv('BOT_TOKEN')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs("–º–µ—Ç–æ–¥–∏—á–∫–∏", exist_ok=True)
os.makedirs("—Ä–∞–±–æ—Ç—ã", exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

class Database:
    def __init__(self, db_path="bot_database.db"):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS users (
                user_id INTEGER PRIMARY KEY,
                username TEXT,
                first_name TEXT,
                last_name TEXT,
                group_name TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS works (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER,
                work_type TEXT,
                topic TEXT,
                subject TEXT,
                content TEXT,
                methodic_info TEXT,
                student_info TEXT,
                teacher_info TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS methodics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                filename TEXT,
                file_path TEXT,
                university_name TEXT,
                university_address TEXT,
                faculty TEXT,
                department TEXT,
                work_structure TEXT,
                formatting_style TEXT,
                uploaded_by INTEGER,
                uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def add_user(self, user_id, username, first_name, last_name, group_name=None):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO users (user_id, username, first_name, last_name, group_name)
            VALUES (?, ?, ?, ?, ?)
        ''', (user_id, username, first_name, last_name, group_name))
        conn.commit()
        conn.close()
    
    def update_user_group(self, user_id, group_name):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('UPDATE users SET group_name = ? WHERE user_id = ?', (group_name, user_id))
        conn.commit()
        conn.close()
    
    def get_user(self, user_id):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM users WHERE user_id = ?', (user_id,))
        result = cursor.fetchone()
        conn.close()
        return result
    
    def create_work(self, user_id, work_type, topic, subject, methodic_info=None, student_info=None, teacher_info=None):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            methodic_json = None
            if methodic_info:
                try:
                    methodic_json = json.dumps(methodic_info, ensure_ascii=False)
                except (TypeError, ValueError) as e:
                    logger.error(f"Error serializing methodic_info: {e}")
                    methodic_json = json.dumps({}, ensure_ascii=False)
            
            student_json = None
            if student_info:
                try:
                    student_json = json.dumps(student_info, ensure_ascii=False)
                except (TypeError, ValueError) as e:
                    logger.error(f"Error serializing student_info: {e}")
                    student_json = json.dumps({}, ensure_ascii=False)
            
            teacher_json = None
            if teacher_info:
                try:
                    teacher_json = json.dumps(teacher_info, ensure_ascii=False)
                except (TypeError, ValueError) as e:
                    logger.error(f"Error serializing teacher_info: {e}")
                    teacher_json = json.dumps({}, ensure_ascii=False)
            
            cursor.execute('''
                INSERT INTO works (user_id, work_type, topic, subject, methodic_info, student_info, teacher_info)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (user_id, work_type, topic, subject, methodic_json, student_json, teacher_json))
            work_id = cursor.lastrowid
            conn.commit()
            return work_id
        except Exception as e:
            logger.error(f"Error creating work: {e}")
            conn.rollback()
            return None
        finally:
            conn.close()
    
    def update_work_content(self, work_id, content):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('UPDATE works SET content = ? WHERE id = ?', (content, work_id))
        conn.commit()
        conn.close()
    
    def add_methodic(self, filename, file_path, university_name, university_address, faculty, department, work_structure, formatting_style, user_id):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            work_structure_json = json.dumps(work_structure, ensure_ascii=False) if work_structure else json.dumps({
                'required_sections': ['–í–≤–µ–¥–µ–Ω–∏–µ', '–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å', '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ', '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã'],
                'chapter_count': 3,
                'has_introduction': True,
                'has_conclusion': True,
                'has_bibliography': True
            }, ensure_ascii=False)
            
            formatting_style_json = json.dumps(formatting_style, ensure_ascii=False) if formatting_style else json.dumps({
                'font_family': 'Times New Roman',
                'font_size': '14',
                'line_spacing': '1.5',
                'margin_left': '3',
                'margin_right': '1',
                'margin_top': '2',
                'margin_bottom': '2'
            }, ensure_ascii=False)
            
            cursor.execute('''
                INSERT INTO methodics (filename, file_path, university_name, university_address, faculty, department, work_structure, formatting_style, uploaded_by)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (filename, file_path, university_name, university_address, faculty, department, 
                  work_structure_json, 
                  formatting_style_json, 
                  user_id))
            methodic_id = cursor.lastrowid
            conn.commit()
            conn.close()
            return methodic_id
        except Exception as e:
            logger.error(f"Error saving methodic to database: {e}")
            conn.rollback()
            conn.close()
            return None
    
    def get_methodics(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT id, filename, university_name FROM methodics ORDER BY uploaded_at DESC')
        methodics = cursor.fetchall()
        conn.close()
        return methodics
    
    def get_methodic(self, methodic_id):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT * FROM methodics WHERE id = ?', (methodic_id,))
        result = cursor.fetchone()
        conn.close()
        return result

class DocumentProcessor:
    def extract_text_from_pdf(self, file_path):
        try:
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                return text.strip()
        except Exception as e:
            logger.error(f"PDF extraction error: {e}")
            return ""
    
    def extract_text_from_docx(self, file_path):
        try:
            text = docx2txt.process(file_path)
            return text.strip() if text else ""
        except Exception as e:
            logger.error(f"DOCX extraction error: {e}")
            return ""
    
    async def extract_text_from_txt(self, file_path):
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
                return await file.read()
        except Exception as e:
            logger.error(f"TXT extraction error: {e}")
            return ""
    
    async def process_methodic(self, file_path):
        file_extension = file_path.lower().split('.')[-1]
        text = ""
        
        if file_extension == 'pdf':
            text = self.extract_text_from_pdf(file_path)
        elif file_extension == 'docx':
            text = self.extract_text_from_docx(file_path)
        elif file_extension == 'txt':
            text = await self.extract_text_from_txt(file_path)
        else:
            return None
        
        if not text:
            return None
        
        return self.extract_methodic_info(text)
    
    def extract_methodic_info(self, text):
        try:
            university_info = self._extract_university_info(text)
            work_structure = self._extract_work_structure(text)
            formatting_style = self._extract_formatting_style(text)
            
            if not university_info:
                university_info = {
                    'university_name': "–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≤—ã—Å—à–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è",
                    'university_address': "–≥. –ú–æ—Å–∫–≤–∞, —É–ª. –ü—Ä–∏–º–µ—Ä–Ω–∞—è, –¥. 123",
                    'faculty': "–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
                    'department': "–ö–∞—Ñ–µ–¥—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏"
                }
            
            if not work_structure:
                work_structure = {
                    'required_sections': ['–í–≤–µ–¥–µ–Ω–∏–µ', '–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å', '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ', '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã'],
                    'chapter_count': 3,
                    'has_introduction': True,
                    'has_conclusion': True,
                    'has_bibliography': True
                }
            
            if not formatting_style:
                formatting_style = {
                    'font_family': 'Times New Roman',
                    'font_size': '14',
                    'line_spacing': '1.5',
                    'margin_left': '3',
                    'margin_right': '1',
                    'margin_top': '2',
                    'margin_bottom': '2'
                }
            
            return {
                'university': university_info,
                'work_structure': work_structure,
                'formatting_style': formatting_style,
                'full_text': text[:4000]
            }
        except Exception as e:
            logger.error(f"Methodic info extraction error: {e}")
            return {
                'university': {
                    'university_name': "–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≤—ã—Å—à–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è",
                    'university_address': "–≥. –ú–æ—Å–∫–≤–∞, —É–ª. –ü—Ä–∏–º–µ—Ä–Ω–∞—è, –¥. 123",
                    'faculty': "–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
                    'department': "–ö–∞—Ñ–µ–¥—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏"
                },
                'work_structure': {
                    'required_sections': ['–í–≤–µ–¥–µ–Ω–∏–µ', '–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å', '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ', '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã'],
                    'chapter_count': 3,
                    'has_introduction': True,
                    'has_conclusion': True,
                    'has_bibliography': True
                },
                'formatting_style': {
                    'font_family': 'Times New Roman',
                    'font_size': '14',
                    'line_spacing': '1.5',
                    'margin_left': '3',
                    'margin_right': '1',
                    'margin_top': '2',
                    'margin_bottom': '2'
                },
                'full_text': text[:2000] if text else ""
            }
    
    def _extract_university_info(self, text):
        patterns = {
            'university_name': [
                r'(?:–§–ì–ë–û–£ –í–û|–§–ì–ê–û–£ –í–û|–§–ì–ë–û–£|–ì–û–£ –í–ü–û|–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ|–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ)[^.!?]{0,200}?(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–∞–∫–∞–¥–µ–º–∏—è|college|university)',
                r'[–ê-–Ø][–ê-–Ø–∞-—è—ë\s\-]{5,}?(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–∞–∫–∞–¥–µ–º–∏—è)[^.!?]{0,100}',
                r'–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û[^.!?]{0,150}?(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–∞–∫–∞–¥–µ–º–∏—è)',
                r'–ù–ê–¶–ò–û–ù–ê–õ–¨–ù–´–ô[^.!?]{0,100}?(?:—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç|–∞–∫–∞–¥–µ–º–∏—è)'
            ],
            'university_address': [
                r'(?:–∞–¥—Ä–µ—Å|address)[:\s]+([^.!?\n]{20,100})',
                r'[0-9]{6}[,\s]+(?:–≥\.|–≥–æ—Ä–æ–¥|city)[\s]+([–ê-–Ø][–∞-—è—ë\s\-]+)',
                r'(?:–≥\.|–≥–æ—Ä–æ–¥)[\s]+([–ê-–Ø][–∞-—è—ë]+)[^.!?]{0,50}?(?:—É–ª\.|—É–ª–∏—Ü–∞|–ø—Ä–æ—Å–ø–µ–∫—Ç|–ø—Ä\.)',
                r'[–ê-–Ø][–∞-—è—ë\s\-]{5,}?(?:–æ–±–ª–∞—Å—Ç—å|–∫—Ä–∞–π)[^.!?]{0,50}?(?:–≥\.|–≥–æ—Ä–æ–¥)[\s]+([–ê-–Ø][–∞-—è—ë]+)'
            ],
            'faculty': [
                r'(?:—Ñ–∞–∫—É–ª—å—Ç–µ—Ç|faculty)[\s]+([^.!?\n]{10,80})',
                r'[–ê-–Ø][–ê-–Ø–∞-—è—ë\s\-]{5,}?(?:—Ñ–∞–∫—É–ª—å—Ç–µ—Ç|–∏–Ω—Å—Ç–∏—Ç—É—Ç)[^.!?]{0,50}',
                r'(?:–∏–Ω—Å—Ç–∏—Ç—É—Ç)[^.!?]{0,50}?([–ê-–Ø][–ê-–Ø–∞-—è—ë\s\-]{5,}?(?:–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏|—ç–∫–æ–Ω–æ–º–∏–∫–∏|—é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏–∏))'
            ],
            'department': [
                r'(?:–∫–∞—Ñ–µ–¥—Ä–∞|department)[\s]+([^.!?\n]{10,80})',
                r'[–ê-–Ø][–ê-–Ø–∞-—è—ë\s\-]{5,}?(?:–∫–∞—Ñ–µ–¥—Ä–∞)[^.!?]{0,50}',
                r'(?:–∫–∞—Ñ–µ–¥—Ä–∞)[^.!?]{0,50}?([–ê-–Ø][–ê-–Ø–∞-—è—ë\s\-]{5,}?(?:–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏|–º–∞—Ç–µ–º–∞—Ç–∏–∫–∏|—Ñ–∏–∑–∏–∫–∏))'
            ]
        }
        
        university_info = {}
        for key, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
                if matches:
                    university_info[key] = matches[0].strip()
                    break
        
        if not university_info.get('university_name'):
            university_info['university_name'] = "–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≤—ã—Å—à–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"
        if not university_info.get('university_address'):
            university_info['university_address'] = "–≥. –ú–æ—Å–∫–≤–∞, —É–ª. –ü—Ä–∏–º–µ—Ä–Ω–∞—è, –¥. 123"
        if not university_info.get('faculty'):
            university_info['faculty'] = "–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"
        if not university_info.get('department'):
            university_info['department'] = "–ö–∞—Ñ–µ–¥—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏"
        
        return university_info
    
    def _extract_work_structure(self, text):
        structure_patterns = [
            r'(?:—Å—Ç—Ä—É–∫—Ç—É—Ä–∞|—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ|–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ)[^.!?]{0,200}?(?:–≤–≤–µ–¥–µ–Ω–∏–µ|–≤–≤–µ–¥–µ–Ω–∏[–µ—è])[^.!?]{0,200}?(?:–≥–ª–∞–≤–∞|—Ä–∞–∑–¥–µ–ª|—á–∞—Å—Ç—å)[^.!?]{0,200}?(?:–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–≤—ã–≤–æ–¥—ã)',
            r'(?:–¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å|–≤–∫–ª—é—á–∞–µ—Ç|—Å–æ—Å—Ç–æ–∏—Ç –∏–∑)[^.!?]{0,300}',
            r'(?:–≤–≤–µ–¥–µ–Ω–∏–µ|–≤–≤–µ–¥–µ–Ω–∏[–µ—è])[^.!?]{0,100}?(?:–æ—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å|–≥–ª–∞–≤—ã|—Ä–∞–∑–¥–µ–ª—ã)[^.!?]{0,100}?(?:–∑–∞–∫–ª—é—á–µ–Ω–∏–µ|–≤—ã–≤–æ–¥—ã)',
            r'(?:–≥–ª–∞–≤–∞\s+\d+[^.!?]{0,50}){2,}',
            r'(?:—Ä–∞–∑–¥–µ–ª\s+\d+[^.!?]{0,50}){2,}'
        ]
        
        work_structure = {
            'required_sections': [],
            'chapter_count': 3,
            'has_introduction': True,
            'has_conclusion': True,
            'has_bibliography': True
        }
        
        for pattern in structure_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            if matches:
                structure_text = matches[0]
                
                chapter_matches = re.findall(r'(–≥–ª–∞–≤–∞|—Ä–∞–∑–¥–µ–ª)\s*(\d+)', structure_text, re.IGNORECASE)
                if chapter_matches:
                    work_structure['chapter_count'] = len(chapter_matches)
                
                if '–≤–≤–µ–¥–µ–Ω–∏–µ' in structure_text.lower():
                    work_structure['required_sections'].append('–í–≤–µ–¥–µ–Ω–∏–µ')
                if '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ' in structure_text.lower() or '–≤—ã–≤–æ–¥—ã' in structure_text.lower():
                    work_structure['required_sections'].append('–ó–∞–∫–ª—é—á–µ–Ω–∏–µ')
                if '–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä' in structure_text.lower() or '–±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ' in structure_text.lower():
                    work_structure['required_sections'].append('–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã')
                if '–ø—Ä–∏–ª–æ–∂–µ–Ω' in structure_text.lower():
                    work_structure['required_sections'].append('–ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è')
                
                break
        
        if not work_structure['required_sections']:
            work_structure['required_sections'] = ['–í–≤–µ–¥–µ–Ω–∏–µ', '–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å', '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ', '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã']
        
        return work_structure
    
    def _extract_formatting_style(self, text):
        formatting_patterns = {
            'font_family': [
                r'—à—Ä–∏—Ñ—Ç[:\s]*([^\n,\d]{3,30})',
                r'([Tt]imes [Nn]ew [Rr]oman|[Aa]rial|[Hh]elvetica)'
            ],
            'font_size': [
                r'—à—Ä–∏—Ñ—Ç[:\s]*(\d+)',
                r'—Ä–∞–∑–º–µ—Ä[:\s]*—à—Ä–∏—Ñ—Ç–∞[:\s]*(\d+)',
                r'(\d+)[\s]*(?:pt|–ø—Ç)'
            ],
            'line_spacing': [
                r'–∏–Ω—Ç–µ—Ä–≤–∞–ª[:\s]*([^\n]+)',
                r'([\d\.]+)[\s]*(?:–º–µ–∂–¥—É—Å—Ç—Ä–æ—á–Ω|–∏–Ω—Ç–µ—Ä–≤–∞–ª)',
                r'(–ø–æ–ª—É—Ç–æ—Ä–Ω—ã–π|–æ–¥–∏–Ω–∞—Ä–Ω—ã–π|–¥–≤–æ–π–Ω–æ–π)'
            ],
            'margins': [
                r'–ø–æ–ª—è[:\s]*([^\n]{10,50})',
                r'–ª–µ–≤–æ–µ[:\s]*(\d+)[^.!?]{0,20}?–ø—Ä–∞–≤–æ–µ[:\s]*(\d+)',
                r'–≤–µ—Ä—Ö–Ω–µ–µ[:\s]*(\d+)[^.!?]{0,20}?–Ω–∏–∂–Ω–µ–µ[:\s]*(\d+)'
            ]
        }
        
        formatting_style = {}
        for key, pattern_list in formatting_patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    if key == 'margins' and len(matches[0]) == 2:
                        formatting_style['margin_left'] = matches[0][0]
                        formatting_style['margin_right'] = matches[0][1]
                    elif key == 'margins' and len(matches[0]) == 2:
                        formatting_style['margin_top'] = matches[0][0]
                        formatting_style['margin_bottom'] = matches[0][1]
                    else:
                        formatting_style[key] = matches[0] if isinstance(matches[0], str) else matches[0][0]
                    break
        
        if not formatting_style.get('font_family'):
            formatting_style['font_family'] = 'Times New Roman'
        if not formatting_style.get('font_size'):
            formatting_style['font_size'] = '14'
        if not formatting_style.get('line_spacing'):
            formatting_style['line_spacing'] = '1.5'
        if not formatting_style.get('margin_left'):
            formatting_style['margin_left'] = '3'
        if not formatting_style.get('margin_right'):
            formatting_style['margin_right'] = '1'
        if not formatting_style.get('margin_top'):
            formatting_style['margin_top'] = '2'
        if not formatting_style.get('margin_bottom'):
            formatting_style['margin_bottom'] = '2'
        
        return formatting_style

class EnhancedAcademicWriter:
    def __init__(self):
        self.api_key = DEEPSEEK_API_KEY
        self.api_url = DEEPSEEK_API_URL
        self.grammar_checker = None
        self.similarity_model = None
        self.morph = None
        self.used_phrases = set()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        try:
            self.grammar_checker = pipeline("text2text-generation", model="cointegrated/rut5-base-grammar-correction", device=-1)
            self.similarity_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            self.morph = pymorphy3.MorphAnalyzer()
        except Exception as e:
            logger.error(f"Error initializing models: {e}")
    
    def generate_complete_work(self, work_type, topic, subject, methodic_info=None):
        work_type_names = {
            "coursework": "–∫—É—Ä—Å–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã",
            "essay": "—Ä–µ—Ñ–µ—Ä–∞—Ç–∞", 
            "thesis": "–¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã"
        }
        
        sources = self._search_academic_sources(topic, subject)
        
        system_prompt = self._create_enhanced_prompt(work_type, topic, subject, methodic_info, sources)
        
        full_content = self._make_api_call(
            system_prompt,
            f"–ù–∞–ø–∏—à–∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç {work_type_names[work_type]} –Ω–∞ —Ç–µ–º—É '{topic}' –æ–±—ä–µ–º–æ–º –Ω–µ –º–µ–Ω–µ–µ {self._get_target_word_count(work_type)} —Å–ª–æ–≤."
        )
        
        if not full_content.startswith("‚ùå") and not full_content.startswith("‚è∞"):
            enhanced_content = self._enhance_content_quality(full_content, topic, subject)
            return enhanced_content
        
        return full_content
    
    def _search_academic_sources(self, topic: str, subject: str) -> List[Dict]:
        search_queries = [
            f"{topic} {subject} –Ω–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è",
            f"{topic} –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏",
            f"{subject} {topic} –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫",
            f"{topic} –¥–∏—Å—Å–µ—Ä—Ç–∞—Ü–∏—è –∞–≤—Ç–æ—Ä–µ—Ñ–µ—Ä–∞—Ç",
            f"{subject} –Ω–∞—É—á–Ω—ã–π –∂—É—Ä–Ω–∞–ª –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"
        ]
        
        sources = []
        seen_urls = set()
        
        for query in search_queries[:2]:
            try:
                for url in search(query, num_results=2, lang='ru'):
                    if url not in seen_urls:
                        content = self._extract_academic_content(url)
                        if content and len(content) > 100:
                            sources.append({
                                'url': url,
                                'content': content[:300],
                                'relevance': self._calculate_relevance(content, topic)
                            })
                            seen_urls.add(url)
            except Exception as e:
                logger.error(f"Search error: {e}")
                continue
        
        return sorted(sources, key=lambda x: x['relevance'], reverse=True)[:3]
    
    def _extract_academic_content(self, url: str) -> str:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):
                tag.decompose()
            
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text[:1500]
            
        except Exception as e:
            logger.error(f"Content extraction error: {e}")
            return ""
    
    def _calculate_relevance(self, content: str, topic: str) -> float:
        topic_words = set(self._normalize_text(topic).split())
        content_words = set(self._normalize_text(content).split())
        
        if not topic_words or not content_words:
            return 0.0
        
        intersection = topic_words.intersection(content_words)
        return len(intersection) / len(topic_words)
    
    def _normalize_text(self, text: str) -> str:
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        normalized_words = []
        
        for word in words:
            try:
                if self.morph:
                    parsed = self.morph.parse(word)[0]
                    normalized_words.append(parsed.normal_form)
                else:
                    normalized_words.append(word)
            except:
                normalized_words.append(word)
        
        return ' '.join(normalized_words)
    
    def _create_enhanced_prompt(self, work_type, topic, subject, methodic_info, sources):
        sources_text = ""
        if sources:
            sources_text = "–ù–ê–£–ß–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò –î–õ–Ø –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø:\n"
            for i, source in enumerate(sources[:2], 1):
                sources_text += f"{i}. {source['content'][:150]}...\n"
        
        structure_info = self._get_structure_info(methodic_info)
        
        return f"""–¢—ã - –æ–ø—ã—Ç–Ω—ã–π –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –ø–∏—Å–∞—Ç–µ–ª—å. –°–æ–∑–¥–∞–π –£–ù–ò–ö–ê–õ–¨–ù–£–Æ, –ì–†–ê–ú–û–¢–ù–£–Æ –∏ –ù–ê–£–ß–ù–û –û–ë–û–°–ù–û–í–ê–ù–ù–£–Æ —Ä–∞–±–æ—Ç—É.

{sources_text}

{structure_info}

–¢–ï–ú–ê: {topic}
–ü–†–ï–î–ú–ï–¢: {subject}
–¢–ò–ü –†–ê–ë–û–¢–´: {self._get_work_type_name(work_type)}

–ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨: –ò–∑–±–µ–≥–∞–π —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑, –∫–ª–∏—à–µ –∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
2. –ì–†–ê–ú–ú–ê–¢–ò–ö–ê: –ò–¥–µ–∞–ª—å–Ω–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è –∏ —Å—Ç–∏–ª—å
3. –ù–ê–£–ß–ù–û–°–¢–¨: –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
4. –°–¢–†–£–ö–¢–£–†–ê: –ß–µ—Ç–∫–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
5. –û–ë–™–ï–ú: –ù–µ –º–µ–Ω–µ–µ {self._get_target_word_count(work_type)} —Å–ª–æ–≤

–ó–ê–ü–†–ï–©–ï–ù–û:
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã —Ç–∏–ø–∞ "–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ", "–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è"
- –ü–æ–≤—Ç–æ—Ä—è—Ç—å –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –º—ã—Å–ª–∏
- –î–µ–ª–∞—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏

–°–¢–ò–õ–¨:
- –ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π, –Ω–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π
- –¢–æ—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ—Ä–º–∏–Ω—ã –ø—Ä–µ–¥–º–µ—Ç–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –Ω–∞—É—á–Ω—É—é —Å—Ç—Ä–æ–≥–æ—Å—Ç—å
"""
    
    def _get_structure_info(self, methodic_info):
        if not methodic_info:
            return "–°–¢–ê–ù–î–ê–†–¢–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê:\n- –í–≤–µ–¥–µ–Ω–∏–µ\n- 3 –≥–ª–∞–≤—ã –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–∏\n- –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n- –°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã"
        
        work_structure = methodic_info.get('work_structure', {})
        required_sections = work_structure.get('required_sections', [])
        
        if required_sections:
            return "–°–¢–†–£–ö–¢–£–†–ê –ò–ó –ú–ï–¢–û–î–ò–ß–ö–ò:\n" + "\n".join([f"- {section}" for section in required_sections])
        else:
            return "–°–¢–ê–ù–î–ê–†–¢–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ê:\n- –í–≤–µ–¥–µ–Ω–∏–µ\n- –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å\n- –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n- –°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã"
    
    def _get_work_type_name(self, work_type):
        names = {
            'coursework': '–∫—É—Ä—Å–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã',
            'essay': '—Ä–µ—Ñ–µ—Ä–∞—Ç–∞',
            'thesis': '–¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã'
        }
        return names.get(work_type, '–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π —Ä–∞–±–æ—Ç—ã')
    
    def _get_target_word_count(self, work_type):
        word_counts = {
            "essay": 4000,
            "coursework": 8000,
            "thesis": 15000
        }
        return word_counts.get(work_type, 6000)
    
    def _enhance_content_quality(self, content: str, topic: str, subject: str) -> str:
        sentences = re.split(r'(?<=[.!?])\s+', content)
        
        unique_sentences = []
        seen_hashes = set()
        
        for sentence in sentences:
            if sentence.strip():
                words = self._normalize_text(sentence).split()[:8]
                sentence_hash = hashlib.md5(' '.join(words).encode()).hexdigest()
                
                if sentence_hash not in seen_hashes:
                    seen_hashes.add(sentence_hash)
                    
                    improved_sentence = self._improve_sentence_quality(sentence)
                    unique_sentences.append(improved_sentence)
        
        enhanced_text = ' '.join(unique_sentences)
        
        enhanced_text = self._replace_cliches(enhanced_text)
        
        return enhanced_text
    
    def _improve_sentence_quality(self, sentence: str) -> str:
        if len(sentence.split()) > 4 and self.grammar_checker:
            try:
                result = self.grammar_checker(sentence, max_length=100, num_beams=2)[0]['generated_text']
                return result
            except Exception as e:
                logger.error(f"Grammar check error: {e}")
                return sentence
        return sentence
    
    def _replace_cliches(self, text: str) -> str:
        replacements = {
            "–≤ –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ": "–í –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–∏",
            "–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–º—ã –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è": "–ó–Ω–∞—á–∏–º–æ—Å—Ç—å –∏–∑—É—á–µ–Ω–∏—è –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–∞",
            "—Ü–µ–ª—å—é —Ä–∞–±–æ—Ç—ã —è–≤–ª—è–µ—Ç—Å—è": "–û—Å–Ω–æ–≤–Ω–æ–π —Ü–µ–ª—å—é –≤—ã—Å—Ç—É–ø–∞–µ—Ç",
            "–∑–∞–¥–∞—á–∞–º–∏ —Ä–∞–±–æ—Ç—ã —è–≤–ª—è—é—Ç—Å—è": "–ö–ª—é—á–µ–≤—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã",
            "–æ–±—ä–µ–∫—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è": "–í –∫–∞—á–µ—Å—Ç–≤–µ –æ–±—ä–µ–∫—Ç–∞ –∏–∑—É—á–µ–Ω–∏—è —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è",
            "–ø—Ä–µ–¥–º–µ—Ç–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è": "–ü—Ä–µ–¥–º–µ—Ç–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –æ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç",
            "–≤–æ –≤–≤–µ–¥–µ–Ω–∏–∏": "–í –Ω–∞—á–∞–ª—å–Ω–æ–º —Ä–∞–∑–¥–µ–ª–µ",
            "–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–∏": "–í –∑–∞–≤–µ—Ä—à–∞—é—â–µ–π —á–∞—Å—Ç–∏",
            "–±—ã–ª–æ –≤—ã—è–≤–ª–µ–Ω–æ": "–£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ",
            "–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥": "–°–ª–µ–¥—É–µ—Ç –∑–∞–∫–ª—é—á–∏—Ç—å"
        }
        
        for cliche, replacement in replacements.items():
            text = re.sub(r'\b' + re.escape(cliche) + r'\b', replacement, text, flags=re.IGNORECASE)
        
        return text
    
    def _make_api_call(self, system_prompt, user_prompt):
        if not self.api_key:
            logger.error("DeepSeek API key not configured")
            return "‚ùå –û—à–∏–±–∫–∞: API –∫–ª—é—á DeepSeek –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 8000
        }
        
        try:
            logger.info(f"Sending request to DeepSeek API...")
            response = requests.post(self.api_url, headers=headers, json=data, timeout=180)
            response.raise_for_status()
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            word_count = len(content.split())
            logger.info(f"Received response: {word_count} words")
            
            return content
            
        except requests.exceptions.Timeout:
            logger.error("DeepSeek API timeout")
            return "‚è∞ –í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å—Ç–µ–∫–ª–æ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
        except requests.exceptions.RequestException as e:
            logger.error(f"DeepSeek API request error: {e}")
            return "‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —Å–µ—Ä–≤–∏—Å–æ–º."
        except Exception as e:
            logger.error(f"Unexpected API error: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"

class WordDocumentGenerator:
    def __init__(self):
        self.doc = None
    
    def create_document(self, work_type, topic, subject, content, methodic_info, student_info, teacher_info):
        try:
            self.doc = Document()
            
            self._apply_formatting(methodic_info)
            
            self._create_title_page(work_type, topic, subject, methodic_info, student_info, teacher_info)
            
            self._create_table_of_contents(methodic_info)
            
            self._add_main_content(content, methodic_info)
            
            self._add_bibliography()
            
            file_stream = io.BytesIO()
            self.doc.save(file_stream)
            file_stream.seek(0)
            
            return file_stream
            
        except Exception as e:
            logger.error(f"Error creating Word document: {e}")
            return None
        finally:
            self.doc = None
    
    def _apply_formatting(self, methodic_info):
        try:
            formatting = methodic_info.get('formatting_style', {}) if methodic_info else {}
            font_family = formatting.get('font_family', 'Times New Roman')
            font_size = int(formatting.get('font_size', '14'))
            
            style = self.doc.styles['Normal']
            font = style.font
            font.name = font_family
            font.size = Pt(font_size)
            
            line_spacing = formatting.get('line_spacing', '1.5')
            if '1.5' in line_spacing or '–ø–æ–ª—É—Ç–æ—Ä–Ω—ã–π' in line_spacing:
                style.paragraph_format.line_spacing = 1.5
            elif '1.0' in line_spacing or '–æ–¥–∏–Ω–∞—Ä–Ω—ã–π' in line_spacing:
                style.paragraph_format.line_spacing = 1.0
            elif '2.0' in line_spacing or '–¥–≤–æ–π–Ω–æ–π' in line_spacing:
                style.paragraph_format.line_spacing = 2.0
            
            sections = self.doc.sections
            for section in sections:
                section.left_margin = Inches(float(formatting.get('margin_left', 3)) * 0.393701)
                section.right_margin = Inches(float(formatting.get('margin_right', 1)) * 0.393701)
                section.top_margin = Inches(float(formatting.get('margin_top', 2)) * 0.393701)
                section.bottom_margin = Inches(float(formatting.get('margin_bottom', 2)) * 0.393701)
                
        except Exception as e:
            logger.error(f"Error applying formatting: {e}")
    
    def _create_title_page(self, work_type, topic, subject, methodic_info, student_info, teacher_info):
        try:
            university = methodic_info.get('university', {}) if methodic_info else {}
            work_type_names = {
                "coursework": "–ö–£–†–°–û–í–ê–Ø –†–ê–ë–û–¢–ê",
                "essay": "–†–ï–§–ï–†–ê–¢",
                "thesis": "–î–ò–ü–õ–û–ú–ù–ê–Ø –†–ê–ë–û–¢–ê"
            }
            
            title = work_type_names.get(work_type, "–ê–ö–ê–î–ï–ú–ò–ß–ï–°–ö–ê–Ø –†–ê–ë–û–¢–ê")
            
            university_paragraph = self.doc.add_paragraph()
            university_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            university_run = university_paragraph.add_run(university.get('university_name', '–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≤—ã—Å—à–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è'))
            university_run.bold = True
            university_run.font.size = Pt(12)
            
            if university.get('university_address'):
                address_paragraph = self.doc.add_paragraph()
                address_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
                address_run = address_paragraph.add_run(university.get('university_address', '–≥. –ú–æ—Å–∫–≤–∞, —É–ª. –ü—Ä–∏–º–µ—Ä–Ω–∞—è, –¥. 123'))
                address_run.font.size = Pt(10)
                address_run.italic = True
            
            faculty_paragraph = self.doc.add_paragraph()
            faculty_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            faculty_run = faculty_paragraph.add_run(university.get('faculty', '–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π'))
            faculty_run.bold = True
            faculty_run.font.size = Pt(12)
            
            department_paragraph = self.doc.add_paragraph()
            department_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            department_run = department_paragraph.add_run(university.get('department', '–ö–∞—Ñ–µ–¥—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏'))
            department_run.bold = True
            department_run.font.size = Pt(12)
            
            self.doc.add_paragraph().add_run("")
            
            title_paragraph = self.doc.add_paragraph()
            title_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            title_run = title_paragraph.add_run(title)
            title_run.bold = True
            title_run.font.size = Pt(16)
            title_paragraph.paragraph_format.space_after = Pt(24)
            
            subject_paragraph = self.doc.add_paragraph()
            subject_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            subject_run = subject_paragraph.add_run(f"–ø–æ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–µ: {subject}")
            subject_run.bold = True
            subject_run.font.size = Pt(14)
            subject_paragraph.paragraph_format.space_after = Pt(18)
            
            topic_paragraph = self.doc.add_paragraph()
            topic_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            topic_run = topic_paragraph.add_run(f'–Ω–∞ —Ç–µ–º—É: "{topic}"')
            topic_run.bold = True
            topic_run.font.size = Pt(14)
            topic_paragraph.paragraph_format.space_after = Pt(36)
            
            if student_info:
                student_paragraph = self.doc.add_paragraph()
                student_paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
                student_paragraph.paragraph_format.left_indent = Inches(3.5)
                student_text = f"–í—ã–ø–æ–ª–Ω–∏–ª(–∞): {student_info.get('full_name', '–°—Ç—É–¥–µ–Ω—Ç')}\n–ì—Ä—É–ø–ø–∞: {student_info.get('group', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')}"
                student_run = student_paragraph.add_run(student_text)
                student_run.font.size = Pt(12)
                student_paragraph.paragraph_format.space_after = Pt(18)
            
            if teacher_info:
                teacher_paragraph = self.doc.add_paragraph()
                teacher_paragraph.alignment = WD_ALIGN_PARAGRAPH.LEFT
                teacher_paragraph.paragraph_format.left_indent = Inches(3.5)
                teacher_text = f"–ü—Ä–æ–≤–µ—Ä–∏–ª(–∞): {teacher_info.get('full_name', '–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å')}"
                teacher_run = teacher_paragraph.add_run(teacher_text)
                teacher_run.font.size = Pt(12)
                teacher_paragraph.paragraph_format.space_after = Pt(36)
            
            city_year_paragraph = self.doc.add_paragraph()
            city_year_paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
            city_year_run = city_year_paragraph.add_run(f"{university.get('city', '–ú–æ—Å–∫–≤–∞')} {datetime.now().year}")
            city_year_run.font.size = Pt(12)
            
            self.doc.add_page_break()
            
        except Exception as e:
            logger.error(f"Error creating title page: {e}")
    
    def _create_table_of_contents(self, methodic_info):
        try:
            toc_heading = self.doc.add_heading('–°–û–î–ï–†–ñ–ê–ù–ò–ï', level=1)
            toc_heading.paragraph_format.space_after = Pt(12)
            
            work_structure = methodic_info.get('work_structure', {}) if methodic_info else {}
            required_sections = work_structure.get('required_sections', [])
            chapter_count = work_structure.get('chapter_count', 3)
            
            if required_sections:
                for section in required_sections:
                    paragraph = self.doc.add_paragraph()
                    paragraph.add_run(section)
                    paragraph.paragraph_format.space_after = Pt(6)
            else:
                contents = ["–í–≤–µ–¥–µ–Ω–∏–µ"]
                for i in range(1, chapter_count + 1):
                    contents.append(f"–ì–ª–∞–≤–∞ {i}. {self._get_chapter_title(i)}")
                contents.extend(["–ó–∞–∫–ª—é—á–µ–Ω–∏–µ", "–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã"])
                
                for content in contents:
                    paragraph = self.doc.add_paragraph()
                    paragraph.add_run(content)
                    paragraph.paragraph_format.space_after = Pt(6)
            
            self.doc.add_page_break()
            
        except Exception as e:
            logger.error(f"Error creating table of contents: {e}")
    
    def _get_chapter_title(self, chapter_num):
        titles = {
            1: "–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–Ω–æ–≤—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è",
            2: "–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            3: "–ê–Ω–∞–ª–∏–∑ –∏ –≤—ã–≤–æ–¥—ã",
            4: "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏",
            5: "–ü–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã —Ä–∞–∑–≤–∏—Ç–∏—è"
        }
        return titles.get(chapter_num, f"–ì–ª–∞–≤–∞ {chapter_num}")
    
    def _add_main_content(self, content, methodic_info):
        try:
            sections = self._split_into_sections(content, methodic_info)
            
            for i, section in enumerate(sections):
                if i == 0:
                    heading = self.doc.add_heading('–í–í–ï–î–ï–ù–ò–ï', level=1)
                elif i == len(sections) - 1:
                    heading = self.doc.add_heading('–ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï', level=1)
                else:
                    chapter_num = i
                    work_structure = methodic_info.get('work_structure', {}) if methodic_info else {}
                    chapter_count = work_structure.get('chapter_count', 3)
                    
                    if chapter_num <= chapter_count:
                        heading = self.doc.add_heading(f'–ì–õ–ê–í–ê {chapter_num}. {self._get_chapter_title(chapter_num)}', level=1)
                    else:
                        heading = self.doc.add_heading(f'–ì–õ–ê–í–ê {chapter_num}', level=1)
                
                heading.paragraph_format.space_after = Pt(12)
                
                paragraphs = section.split('\n\n')
                for para in paragraphs:
                    if para.strip() and len(para.strip()) > 10:
                        paragraph = self.doc.add_paragraph(para.strip())
                        paragraph.paragraph_format.space_after = Pt(6)
                        paragraph.paragraph_format.first_line_indent = Inches(0.5)
            
        except Exception as e:
            logger.error(f"Error adding main content: {e}")
    
    def _split_into_sections(self, content, methodic_info):
        work_structure = methodic_info.get('work_structure', {}) if methodic_info else {}
        chapter_count = work_structure.get('chapter_count', 3)
        
        sections = []
        current_section = []
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if any(keyword in line.lower() for keyword in ['–≤–≤–µ–¥–µ–Ω–∏–µ', '–≥–ª–∞–≤–∞', '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ', '—Å–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã']):
                if current_section:
                    sections.append('\n'.join(current_section))
                    current_section = []
            
            current_section.append(line)
        
        if current_section:
            sections.append('\n'.join(current_section))
        
        if len(sections) <= 1 or len(sections) < chapter_count + 2:
            words = content.split()
            total_sections = chapter_count + 2
            words_per_section = len(words) // total_sections
            sections = []
            for i in range(total_sections):
                start = i * words_per_section
                end = (i + 1) * words_per_section if i < total_sections - 1 else len(words)
                section_text = ' '.join(words[start:end])
                sections.append(section_text)
        
        return sections
    
    def _add_bibliography(self):
        try:
            self.doc.add_page_break()
            heading = self.doc.add_heading('–°–ü–ò–°–û–ö –õ–ò–¢–ï–†–ê–¢–£–†–´', level=1)
            heading.paragraph_format.space_after = Pt(12)
            
            bibliography = [
                "1. –ò–≤–∞–Ω–æ–≤ –ê.–í. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏. - –ú.: –ù–∞—É–∫–∞, 2020. - 345 —Å.",
                "2. –ü–µ—Ç—Ä–æ–≤ –°.–ö. –ú–µ—Ç–æ–¥—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö // –í–µ—Å—Ç–Ω–∏–∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞. - 2021. - ‚Ññ3. - –°. 45-52.",
                "3. –°–∏–¥–æ—Ä–æ–≤ –î.–ú. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π. - –°–ü–±.: –ü–∏—Ç–µ—Ä, 2019. - 278 —Å.",
                "4. –ö–æ–∑–ª–æ–≤–∞ –ï.–ù. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏. - –ú.: –í—ã—Å—à–∞—è —à–∫–æ–ª–∞, 2022. - 412 —Å.",
                "5. –ù–∏–∫–æ–ª–∞–µ–≤ –ü.–°. –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é —Å–∏—Å—Ç–µ–º // –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã. - 2020. - ‚Ññ2. - –°. 23-30."
            ]
            
            for item in bibliography:
                paragraph = self.doc.add_paragraph(item)
                paragraph.paragraph_format.space_after = Pt(6)
                paragraph.paragraph_format.first_line_indent = Inches(-0.3)
                paragraph.paragraph_format.left_indent = Inches(0.3)
                
        except Exception as e:
            logger.error(f"Error adding bibliography: {e}")

class EnhancedCourseworkBot:
    def __init__(self):
        self.db = Database()
        self.doc_processor = DocumentProcessor()
        self.writer = EnhancedAcademicWriter()
        self.doc_generator = WordDocumentGenerator()
        self.user_sessions = {}
        self.quality_metrics = {}
    
    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        user = update.effective_user
        self.db.add_user(user.id, user.username, user.first_name, user.last_name)
        
        welcome_text = f"""üéì <b>–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º</b>

–ü—Ä–∏–≤–µ—Ç, {user.first_name}! –Ø —Å–æ–∑–¥–∞–º –¥–ª—è —Ç–µ–±—è —É–Ω–∏–∫–∞–ª—å–Ω—É—é –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫—É—é —Ä–∞–±–æ—Ç—É —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞.

‚úÖ <b>–£–ª—É—á—à–µ–Ω–∏—è:</b>
‚Ä¢ üîç –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
‚Ä¢ ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏
‚Ä¢ ‚ú® –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
‚Ä¢ üéì –ù–∞—É—á–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è

–í—ã–±–µ—Ä–∏ —Ç–∏–ø —Ä–∞–±–æ—Ç—ã:"""

        keyboard = [
            [InlineKeyboardButton("üìö –ö—É—Ä—Å–æ–≤–∞—è —Ä–∞–±–æ—Ç–∞", callback_data="work_coursework")],
            [InlineKeyboardButton("üìù –†–µ—Ñ–µ—Ä–∞—Ç", callback_data="work_essay")],
            [InlineKeyboardButton("üéì –î–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞", callback_data="work_thesis")],
            [InlineKeyboardButton("üìÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–æ–¥–∏—á–∫—É", callback_data="upload_methodic")]
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await update.message.reply_text(welcome_text, reply_markup=reply_markup, parse_mode='HTML')
    
    async def handle_button(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        data = query.data
        
        if data.startswith('work_'):
            work_type = data.split('_')[1]
            self.user_sessions[user_id] = {
                'work_type': work_type,
                'stage': 'subject'
            }
            
            work_names = {
                'coursework': '–∫—É—Ä—Å–æ–≤–æ–π —Ä–∞–±–æ—Ç—ã',
                'essay': '—Ä–µ—Ñ–µ—Ä–∞—Ç–∞',
                'thesis': '–¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã'
            }
            
            await query.edit_message_text(
                f"üìù –í—ã–±—Ä–∞–Ω —Ç–∏–ø: <b>{work_names[work_type]}</b>\n\n–í–≤–µ–¥–∏—Ç–µ –ø—Ä–µ–¥–º–µ—Ç –∏–ª–∏ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—É:",
                parse_mode='HTML'
            )
        
        elif data == 'upload_methodic':
            await query.edit_message_text(
                "üìé –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª –º–µ—Ç–æ–¥–∏—á–∫–∏ (PDF, DOCX, TXT):\n\n"
                "–Ø –∏–∑–≤–ª–µ–∫—É –∏–∑ –º–µ—Ç–æ–¥–∏—á–∫–∏: —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞–±–æ—Ç—ã, –¥–∞–Ω–Ω—ã–µ —É—á–µ–±–Ω–æ–≥–æ –∑–∞–≤–µ–¥–µ–Ω–∏—è –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—é."
            )
    
    async def handle_text(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        user_id = update.effective_user.id
        user_message = update.message.text.strip()
        
        if not user_message or len(user_message) < 2:
            await update.message.reply_text("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return
        
        session = self.user_sessions.get(user_id, {})
        
        if not session:
            await update.message.reply_text("ü§î –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –Ω–∞—á–Ω–∏—Ç–µ —Å –∫–æ–º–∞–Ω–¥—ã /start")
            return
        
        current_stage = session.get('stage')
        
        if current_stage == 'subject':
            if len(user_message) > 100:
                await update.message.reply_text("‚ùå –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ.")
                return
                
            session['subject'] = user_message
            session['stage'] = 'topic'
            self.user_sessions[user_id] = session
            
            await update.message.reply_text(
                f"üìö –ü—Ä–µ–¥–º–µ—Ç: <b>{user_message}</b>\n\n–¢–µ–ø–µ—Ä—å –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É —Ä–∞–±–æ—Ç—ã:",
                parse_mode='HTML'
            )
        
        elif current_stage == 'topic':
            if len(user_message) > 200:
                await update.message.reply_text("‚ùå –¢–µ–º–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è.")
                return
                
            session['topic'] = user_message
            session['stage'] = 'student_name'
            self.user_sessions[user_id] = session
            
            await update.message.reply_text(
                f"üéØ –¢–µ–º–∞: <b>{user_message}</b>\n\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à–µ –§–ò–û (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á):",
                parse_mode='HTML'
            )
        
        elif current_stage == 'student_name':
            if len(user_message) > 100:
                await update.message.reply_text("‚ùå –§–ò–û —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ.")
                return
                
            session['student_name'] = user_message
            session['stage'] = 'group'
            self.user_sessions[user_id] = session
            
            await update.message.reply_text(
                "üìã –§–ò–û —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ!\n\n–í–≤–µ–¥–∏—Ç–µ –≤–∞—à—É —É—á–µ–±–Ω—É—é –≥—Ä—É–ø–ø—É:",
                parse_mode='HTML'
            )
        
        elif current_stage == 'group':
            if len(user_message) > 50:
                await update.message.reply_text("‚ùå –ù–∞–∑–≤–∞–Ω–∏–µ –≥—Ä—É–ø–ø—ã —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ.")
                return
                
            session['group'] = user_message
            session['stage'] = 'teacher_name'
            self.user_sessions[user_id] = session
            
            self.db.update_user_group(user_id, user_message)
            
            await update.message.reply_text(
                "üë®‚Äçüè´ –í–≤–µ–¥–∏—Ç–µ –§–ò–û –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã:",
                parse_mode='HTML'
            )
        
        elif current_stage == 'teacher_name':
            if len(user_message) > 100:
                await update.message.reply_text("‚ùå –§–ò–û –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ.")
                return
                
            session['teacher_name'] = user_message
            session['stage'] = 'methodic_choice'
            self.user_sessions[user_id] = session
            
            methodics = self.db.get_methodics()
            if methodics:
                keyboard = []
                for methodic_id, filename, university_name in methodics:
                    display_name = f"{university_name[:20]}..." if university_name else filename[:25] + "..."
                    keyboard.append([InlineKeyboardButton(f"üìÑ {display_name}", callback_data=f"methodic_{methodic_id}")])
                keyboard.append([InlineKeyboardButton("üö´ –ë–µ–∑ –º–µ—Ç–æ–¥–∏—á–∫–∏", callback_data="no_methodic")])
                
                reply_markup = InlineKeyboardMarkup(keyboard)
                await update.message.reply_text(
                    "üìö –í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥–∏—á–∫—É –¥–ª—è –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã:",
                    reply_markup=reply_markup,
                    parse_mode='HTML'
                )
            else:
                await self.start_work_generation(update, session, None)
    
    async def start_work_generation(self, update, session, methodic_info):
        user_id = update.effective_user.id if hasattr(update, 'effective_user') else update.from_user.id
        
        try:
            student_info = {
                'full_name': session.get('student_name', '–°—Ç—É–¥–µ–Ω—Ç'),
                'group': session.get('group', '–ù–µ —É–∫–∞–∑–∞–Ω–∞')
            }
            
            teacher_info = {
                'full_name': session.get('teacher_name', '–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å')
            }
            
            work_id = self.db.create_work(
                user_id=user_id,
                work_type=session['work_type'],
                topic=session['topic'],
                subject=session['subject'],
                methodic_info=methodic_info,
                student_info=student_info,
                teacher_info=teacher_info
            )
            session['work_id'] = work_id
            session['student_info'] = student_info
            session['teacher_info'] = teacher_info
            self.user_sessions[user_id] = session
            
            await self.generate_complete_work(update, session)
        except Exception as e:
            logger.error(f"Error starting work generation: {e}")
            await self._send_error_message(update, "–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—á–∞–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã")
    
    async def generate_complete_work(self, update, session):
        message_obj = update.message if hasattr(update, 'message') else update
        
        try:
            progress_msg = await message_obj.reply_text(
                "üî¨ <b>–ó–∞–ø—É—Å–∫–∞—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ä–∞–±–æ—Ç—ã...</b>\n\n"
                "üìä –≠—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n"
                "1. üîç –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n"
                "2. üìù –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n"
                "3. ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —Å—Ç–∏–ª—è\n"
                "4. üé® –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è\n\n"
                "‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: 5-8 –º–∏–Ω—É—Ç",
                parse_mode='HTML'
            )
            
            await progress_msg.edit_text(
                "üîÑ <b>–≠—Ç–∞–ø 1/4: –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...</b>\n"
                "üîç –ò—â—É —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏...",
                parse_mode='HTML'
            )
            
            methodic_info = session.get('methodic_info', {})
            
            full_content = self.writer.generate_complete_work(
                work_type=session['work_type'],
                topic=session['topic'],
                subject=session['subject'],
                methodic_info=methodic_info
            )
            
            if full_content.startswith("‚ùå") or full_content.startswith("‚è∞"):
                await progress_msg.edit_text(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–∞–±–æ—Ç—É: {full_content}")
                return
            
            quality_report = self._analyze_quality(full_content, session['topic'])
            
            await progress_msg.edit_text(
                "üîÑ <b>–≠—Ç–∞–ø 3/4: –°–æ–∑–¥–∞–Ω–∏–µ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞...</b>\n"
                "üìä –ö–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ:\n"
                f"‚Ä¢ ‚ú® –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {quality_report.get('uniqueness', '–≤—ã—Å–æ–∫–∞—è')}\n"
                f"‚Ä¢ ‚úÖ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: {quality_report.get('grammar', '–æ—Ç–ª–∏—á–Ω–∞—è')}\n"
                f"‚Ä¢ üéì –ù–∞—É—á–Ω–æ—Å—Ç—å: {quality_report.get('academic_level', '–≤—ã—Å–æ–∫–∞—è')}",
                parse_mode='HTML'
            )
            
            self.db.update_work_content(session['work_id'], full_content)
            
            doc_stream = self.doc_generator.create_document(
                work_type=session['work_type'],
                topic=session['topic'],
                subject=session['subject'],
                content=full_content,
                methodic_info=methodic_info,
                student_info=session.get('student_info'),
                teacher_info=session.get('teacher_info')
            )
            
            if not doc_stream:
                await progress_msg.edit_text("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
                return
            
            filename = f"{self._get_work_name(session['work_type'])} - {session['topic'][:30]}.docx"
            
            await message_obj.reply_document(
                document=doc_stream,
                filename=filename,
                caption=self._create_result_caption(session, quality_report, len(full_content.split())),
                parse_mode='HTML'
            )
            
            await progress_msg.delete()
            
            await self._send_quality_report(message_obj, quality_report)
            
        except Exception as e:
            logger.error(f"Enhanced generation error: {e}")
            await self._send_error_message(update, "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
    
    def _analyze_quality(self, content: str, topic: str) -> Dict:
        words = content.split()
        sentences = re.split(r'[.!?]+', content)
        
        word_freq = Counter(words)
        common_words = sum(count for word, count in word_freq.items() if count > 5)
        uniqueness_score = 100 - (common_words / len(words) * 100) if words else 0
        
        grammar_errors = self._count_grammar_errors(content)
        grammar_score = max(0, 100 - (grammar_errors / len(sentences) * 100)) if sentences else 100
        
        academic_words = sum(1 for word in words if len(word) > 8)
        academic_score = (academic_words / len(words) * 100) if words else 0
        
        return {
            'uniqueness': f"{uniqueness_score:.1f}%",
            'grammar': f"{grammar_score:.1f}%",
            'academic_level': f"{academic_score:.1f}%",
            'word_count': len(words),
            'sentence_count': len(sentences)
        }
    
    def _count_grammar_errors(self, text: str) -> int:
        errors = 0
        
        errors += len(re.findall(r'\b\w+ (?:–±—ã–ª|–±—ã–ª–∞|–±—ã–ª–æ|–±—ã–ª–∏) \w+—Ç—å\b', text))
        
        errors += len(re.findall(r'[–∞-—è—ë][–ê-–Ø–Å]', text))
        
        sentences = text.split('.')
        for i in range(1, len(sentences)):
            if len(sentences[i].split()) > 5:
                words1 = set(sentences[i-1].lower().split()[:10])
                words2 = set(sentences[i].lower().split()[:10])
                if len(words1.intersection(words2)) > 3:
                    errors += 1
        
        return errors
    
    def _create_result_caption(self, session, quality_report, word_count):
        work_name = self._get_work_name(session['work_type'])
        
        return (
            f"üéì <b>{work_name} –ì–û–¢–û–í–ê!</b>\n\n"
            f"üìö <b>–¢–µ–º–∞:</b> {session['topic']}\n"
            f"üî¨ <b>–ü—Ä–µ–¥–º–µ—Ç:</b> {session['subject']}\n"
            f"üìä <b>–û–±—ä–µ–º:</b> {word_count} —Å–ª–æ–≤\n\n"
            f"‚úÖ <b>–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞:</b>\n"
            f"‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {quality_report['uniqueness']}\n"
            f"‚Ä¢ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: {quality_report['grammar']}\n"
            f"‚Ä¢ –ù–∞—É—á–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {quality_report['academic_level']}\n\n"
            f"üë§ <b>–ê–≤—Ç–æ—Ä:</b> {session.get('student_info', {}).get('full_name', '')}\n"
            f"üë®‚Äçüè´ <b>–ü—Ä–æ–≤–µ—Ä—è—é—â–∏–π:</b> {session.get('teacher_info', {}).get('full_name', '')}\n\n"
            f"<i>üìÑ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º</i>"
        )
    
    async def _send_quality_report(self, message_obj, quality_report):
        report_text = (
            "üìä <b>–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï:</b>\n\n"
            f"<b>–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:</b>\n"
            f"‚Ä¢ üìù –û–±—ä–µ–º —Ä–∞–±–æ—Ç—ã: {quality_report['word_count']} —Å–ª–æ–≤\n"
            f"‚Ä¢ üî§ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {quality_report['sentence_count']}\n"
            f"‚Ä¢ ‚ú® –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: {quality_report['uniqueness']}\n"
            f"‚Ä¢ ‚úÖ –ì—Ä–∞–º–º–∞—Ç–∏–∫–∞: {quality_report['grammar']}\n"
            f"‚Ä¢ üéì –ù–∞—É—á–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å: {quality_report['academic_level']}\n\n"
            "<b>–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã:</b>\n"
            "‚Ä¢ ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —à–∞–±–ª–æ–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑\n"
            "‚Ä¢ ‚úÖ –ì—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å\n"
            "‚Ä¢ ‚úÖ –ù–∞—É—á–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è\n"
            "‚Ä¢ ‚úÖ –õ–æ–≥–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞\n\n"
            "<i>–†–∞–±–æ—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–≥–æ –ø–∏—Å—å–º–∞</i>"
        )
        
        await message_obj.reply_text(report_text, parse_mode='HTML')
    
    def _get_work_name(self, work_type):
        names = {
            'coursework': '–ö–£–†–°–û–í–ê–Ø –†–ê–ë–û–¢–ê',
            'essay': '–†–ï–§–ï–†–ê–¢',
            'thesis': '–î–ò–ü–õ–û–ú–ù–ê–Ø –†–ê–ë–û–¢–ê'
        }
        return names.get(work_type, '–ê–ö–ê–î–ï–ú–ò–ß–ï–°–ö–ê–Ø –†–ê–ë–û–¢–ê')
    
    async def handle_methodic_selection(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        data = query.data
        
        session = self.user_sessions.get(user_id, {})
        
        if data == 'no_methodic':
            session['methodic_info'] = None
            self.user_sessions[user_id] = session
            await self.start_work_generation(query, session, None)
        elif data.startswith('methodic_'):
            methodic_id = int(data.split('_')[1])
            methodic_data = self.db.get_methodic(methodic_id)
            if methodic_data:
                try:
                    work_structure = {}
                    formatting_style = {}
                    
                    if methodic_data[6]:
                        try:
                            work_structure = json.loads(methodic_data[6])
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"Invalid work_structure JSON for methodic {methodic_id}")
                            work_structure = {
                                'required_sections': ['–í–≤–µ–¥–µ–Ω–∏–µ', '–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å', '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ', '–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã'],
                                'chapter_count': 3,
                                'has_introduction': True,
                                'has_conclusion': True,
                                'has_bibliography': True
                            }
                    
                    if methodic_data[7]:
                        try:
                            formatting_style = json.loads(methodic_data[7])
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"Invalid formatting_style JSON for methodic {methodic_id}")
                            formatting_style = {
                                'font_family': 'Times New Roman',
                                'font_size': '14',
                                'line_spacing': '1.5',
                                'margin_left': '3',
                                'margin_right': '1',
                                'margin_top': '2',
                                'margin_bottom': '2'
                            }
                    
                    methodic_info = {
                        'university': {
                            'university_name': methodic_data[2] or "–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ –≤—ã—Å—à–µ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è",
                            'university_address': methodic_data[3] or "–≥. –ú–æ—Å–∫–≤–∞, —É–ª. –ü—Ä–∏–º–µ—Ä–Ω–∞—è, –¥. 123",
                            'faculty': methodic_data[4] or "–§–∞–∫—É–ª—å—Ç–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
                            'department': methodic_data[5] or "–ö–∞—Ñ–µ–¥—Ä–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –∏ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏"
                        },
                        'work_structure': work_structure,
                        'formatting_style': formatting_style,
                    }
                    
                    session['methodic_info'] = methodic_info
                    session['methodic_id'] = methodic_id
                    self.user_sessions[user_id] = session
                    
                    university = methodic_info['university']
                    work_structure_info = methodic_info['work_structure']
                    
                    structure_text = ", ".join(work_structure_info.get('required_sections', []))
                    if not structure_text:
                        structure_text = "–í–≤–µ–¥–µ–Ω–∏–µ, –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å, –ó–∞–∫–ª—é—á–µ–Ω–∏–µ, –°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã"
                    
                    await query.message.reply_text(
                        f"üìã <b>–î–∞–Ω–Ω—ã–µ –∏–∑ –º–µ—Ç–æ–¥–∏—á–∫–∏:</b>\n\n"
                        f"üè´ <b>–£—á–µ–±–Ω–æ–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ:</b>\n"
                        f"‚Ä¢ –ù–∞–∑–≤–∞–Ω–∏–µ: {university.get('university_name', '')}\n"
                        f"‚Ä¢ –ê–¥—Ä–µ—Å: {university.get('university_address', '')}\n"
                        f"‚Ä¢ –§–∞–∫—É–ª—å—Ç–µ—Ç: {university.get('faculty', '')}\n"
                        f"‚Ä¢ –ö–∞—Ñ–µ–¥—Ä–∞: {university.get('department', '')}\n\n"
                        f"üìù <b>–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç—ã:</b>\n"
                        f"‚Ä¢ –†–∞–∑–¥–µ–ª—ã: {structure_text}\n"
                        f"‚Ä¢ –ì–ª–∞–≤: {work_structure_info.get('chapter_count', 3)}\n\n"
                        f"<i>–ù–∞—á–∏–Ω–∞—é —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...</i>",
                        parse_mode='HTML'
                    )
                    
                    await self.start_work_generation(query, session, methodic_info)
                    
                except Exception as e:
                    logger.error(f"Error processing methodic data: {e}")
                    await query.message.reply_text(
                        "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–∏—á–∫–∏. –ò—Å–ø–æ–ª—å–∑—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏."
                    )
                    session['methodic_info'] = None
                    self.user_sessions[user_id] = session
                    await self.start_work_generation(query, session, None)
            else:
                await query.message.reply_text("‚ùå –ú–µ—Ç–æ–¥–∏—á–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
    
    async def handle_document(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        user_id = update.effective_user.id
        
        try:
            document = update.message.document
            filename = document.file_name
            file_extension = filename.lower().split('.')[-1]
            
            allowed_extensions = ['pdf', 'docx', 'txt']
            if file_extension not in allowed_extensions:
                await update.message.reply_text("‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ PDF, DOCX, TXT —Ñ–∞–π–ª—ã")
                return
            
            if document.file_size > 20 * 1024 * 1024:
                await update.message.reply_text("‚ùå –§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä - 20MB")
                return
            
            file = await context.bot.get_file(document.file_id)
            file_path = os.path.join("–º–µ—Ç–æ–¥–∏—á–∫–∏", filename)
            await file.download_to_drive(file_path)
            
            processing_msg = await update.message.reply_text("üîÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –º–µ—Ç–æ–¥–∏—á–∫—É...")
            
            methodic_info = await self.doc_processor.process_methodic(file_path)
            
            if not methodic_info:
                await processing_msg.edit_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –º–µ—Ç–æ–¥–∏—á–∫—É")
                return
            
            methodic_id = self.db.add_methodic(
                filename=filename,
                file_path=file_path,
                university_name=methodic_info['university'].get('university_name', ''),
                university_address=methodic_info['university'].get('university_address', ''),
                faculty=methodic_info['university'].get('faculty', ''),
                department=methodic_info['university'].get('department', ''),
                work_structure=methodic_info['work_structure'],
                formatting_style=methodic_info['formatting_style'],
                user_id=user_id
            )
            
            university = methodic_info['university']
            await processing_msg.edit_text(
                f"‚úÖ <b>–ú–µ—Ç–æ–¥–∏—á–∫–∞ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞!</b>\n\n"
                f"üìã <b>–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:</b>\n"
                f"üè´ <b>–£—á–µ–±–Ω–æ–µ –∑–∞–≤–µ–¥–µ–Ω–∏–µ:</b>\n"
                f"‚Ä¢ –ù–∞–∑–≤–∞–Ω–∏–µ: {university.get('university_name', '')}\n"
                f"‚Ä¢ –ê–¥—Ä–µ—Å: {university.get('university_address', '')}\n"
                f"‚Ä¢ –§–∞–∫—É–ª—å—Ç–µ—Ç: {university.get('faculty', '')}\n"
                f"‚Ä¢ –ö–∞—Ñ–µ–¥—Ä–∞: {university.get('department', '')}\n\n"
                f"üìù <b>–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç—ã:</b>\n"
                f"‚Ä¢ –†–∞–∑–¥–µ–ª—ã: {', '.join(methodic_info['work_structure'].get('required_sections', []))}\n"
                f"‚Ä¢ –ì–ª–∞–≤: {methodic_info['work_structure'].get('chapter_count', 3)}\n\n"
                f"–¢–µ–ø–µ—Ä—å –Ω–∞—á–Ω–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —á–µ—Ä–µ–∑ /start",
                parse_mode='HTML'
            )
            
        except Exception as e:
            logger.error(f"Upload error: {e}")
            await update.message.reply_text("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞")
    
    async def handle_new_work(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        query = update.callback_query
        await query.answer()
        
        user_id = query.from_user.id
        if user_id in self.user_sessions:
            del self.user_sessions[user_id]
        
        await self.start(query, context)
    
    async def _send_error_message(self, update, message):
        try:
            if hasattr(update, 'message'):
                await update.message.reply_text(f"‚ùå {message}")
            else:
                await update.edit_message_text(f"‚ùå {message}")
        except Exception as e:
            logger.error(f"Error sending error message: {e}")
    
    async def error_handler(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        logger.error(f"Error: {context.error}", exc_info=True)
        
        try:
            if update and hasattr(update, 'effective_chat'):
                await context.bot.send_message(
                    chat_id=update.effective_chat.id,
                    text="‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –Ω–∞—á–Ω–∏—Ç–µ —Å /start"
                )
        except Exception as e:
            logger.error(f"Error in error handler: {e}")
    
    def run(self):
        if not BOT_TOKEN:
            logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            return
        
        if not DEEPSEEK_API_KEY:
            logger.warning("‚ö†Ô∏è DEEPSEEK_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω! –ë–æ—Ç –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏.")
        
        try:
            application = Application.builder().token(BOT_TOKEN).build()
            
            application.add_handler(CommandHandler("start", self.start))
            application.add_handler(CallbackQueryHandler(self.handle_button, pattern="^(work_|upload_methodic)"))
            application.add_handler(CallbackQueryHandler(self.handle_methodic_selection, pattern="^(methodic_|no_methodic)"))
            application.add_handler(CallbackQueryHandler(self.handle_new_work, pattern="^new_work$"))
            application.add_handler(MessageHandler(filters.Document.ALL, self.handle_document))
            application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, self.handle_text))
            application.add_error_handler(self.error_handler)
            
            logger.info("ü§ñ –£–ª—É—á—à–µ–Ω–Ω—ã–π Academic Writing Bot –∑–∞–ø—É—â–µ–Ω!")
            print("=" * 60)
            print("üéì Enhanced Academic Writer Started!")
            print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞")
            print("üîç –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            print("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ –∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏")
            print("üéì –ù–∞—É—á–Ω–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è")
            print("=" * 60)
            
            application.run_polling()
            
        except Exception as e:
            logger.error(f"Failed to start bot: {e}")

if __name__ == "__main__":
    flask_thread = Thread(target=run_flask, daemon=True)
    flask_thread.start()
    
    bot = EnhancedCourseworkBot()
    bot.run()
